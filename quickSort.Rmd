---
title: "Quick sort analysis"
author: "Mariam Ahhttouche"
date: "January, 2023"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(corrplot)
library(DoE.wrapper)
```


## Download Raw Data from the website

```{r}
df <- read.csv("data/sama_2014-10-13/measurements_03:47.csv",header=T)
```

```{r}
df
dplyr::glimpse(df)
```

We notice in the Type column that there are empty spaces in the values so we need to fix this by trimming all strings in the dataset:

```{r}
df <- df %>% mutate_if(is.character, str_trim)
dplyr::glimpse(df)
```

```{r}
ggplot() +               
  geom_line(data = df, aes(x=Size, y=Time, color = Type))
```

We can see better the scaling of each type as we increase the size, however the measurements have very big intervals between them so the lines might not really match with the reality.

We plot the confidence intervals for Size and Type features:

```{r}
df$Size_F = as.factor(df$Size)
df$Type_F = as.factor(df$Type)
```


```{r}
with(df, plot(Time~Size_F))
with(df, plot(Time~Type_F))
```

From these confidence intervals we can not compare the different sizes since there confidence interval are interleaving. We can also see that we have a lot of outliers in Time/Type plot.

We want to see how big is the impact of the type of the quicksort run and the size of the problem, so we will feed these two variables into a linear regression model. To do that we first add new variables into the dataset that represent the three different categories of the Type column, since Type is a categorical variable.

```{r}
# Create dummy variable
df$Parallel <- ifelse(df$Type == "Parallel", 1, 0)
df$Sequential <- ifelse(df$Type == "Sequential", 1, 0)
df$Built_In <- ifelse(df$Type == "Built-in", 1, 0)
df
```
```{r}
dfNum=df[, c(1,6,7,8)]
mat.cor=cor(dfNum)
corrplot(mat.cor, type="upper")
```

If we include all three new variables in our model we will get an error since the three are perfectly collinear, so we will omit one of them which will be a reference level for the other variiables we keep. This means that the mean of Parallel and 

```{r}
reg3 = lm(Time~Type_F+Size, data=df)
summary(reg3)
anova(reg3)
```

Using a factor field will give us the same results as when we use dummy variables, however I prefer to use dummy variables since they are more easy to reason with

```{r}
reg1 = lm(Time~Size+Parallel+Sequential, data=df)
summary(reg1)
anova(reg1)
```

From the coefficients T values, we can see that the type feature is not considered as important by the model which is wrong, and this is probably due to the fact that we do not have enough data to infer relationship between time and type.

```{r}
reg0 = lm(Time ~ 1, data = df)
step(reg0, scope=Time~Size+Parallel+Sequential+Built_In, direction="forward")
```

Similarly to the previous result the model with just the Size feature is considered the best one.

```{r}
reg4 = lm(Time~Size, data=df)
pred = predict(reg4, data.frame(Size=100),interval="prediction")
pred
```
```{r}
confidence_interval = 0.04194724+0.0314921
confidence_interval
```
```{r}
par(mfcol=c(2,2))
plot(reg1)
plot(reg1,4)
```

Although the first model has a good R square, we can see from the above plots that it's not a good model, in the cook's distance for example we have many outliers, and we can see from the Residuals vs Fitted plot that the biggest outliers happen when we have big values of Time which happens when we have a big size and that's when we have a significant difference in execution times between the different types of quicksort.


## Using New generated data

To improve the model we will need to have more data, so we start by doing the tests on two different machines, my machine and mandelbrot.
I start by testing on my local machine and I do three different measurements at different times to reduce the impact of the time at which we run the experiment 

```{r}
local_df <- list.files(path = "./data/debian_2023-01-26/", pattern = "*.csv", full.names = T) %>% map_df(~read_csv(.)) 
local_df <- local_df %>% mutate_if(is.character, str_trim)
dplyr::glimpse(local_df)
local_df
```

```{r}
ggplot() +               
  geom_line(data = local_df, aes(x=Size, y=Time, color = Type))
```
```{r}
local_df$Size_F = as.factor(local_df$Size)
local_df$Type_F = as.factor(local_df$Type)
```


```{r}
with(local_df, plot(Time~Size_F))
with(local_df, plot(Time~Type_F))
```

The result here is pretty similar to the results of the original experiment except we know have smaller confidence intervals since we added more experiments, however we can see that we still have a lot of outliers.

```{r}
reg_test_local = lm(Time ~ 1, data = local_df)
step(reg_test_local, scope=Time~Size+Type_F, direction="forward")
```

The resulting model is much better here since it takes into account the type of the algorithm, and it gives the sequential type a much higher coefficient than the parallel one, which matches the result when we have a size of 1000000 but for smaller size the execution time is higher for parallel than sequential or Built-in.

```{r}
reg_local = lm(Time~Size+Type_F, data=local_df)
summary(reg_local)
anova(reg_local)
```

Here we see that the model rejects the sequential and built-in types (value for this one is included in the intercept), which is probably due to the fact that these two types have pretty close execution times in different sizes.  

We do the same thing but this time on mandelbrot:

```{r}
mandelbrot_df <- list.files(path = "./data/im2ag-mandelbrot_2023-01-26/", pattern = "*.csv", full.names = T) %>% map_df(~read_csv(.)) 
mandelbrot_df <- mandelbrot_df %>% mutate_if(is.character, str_trim)
dplyr::glimpse(mandelbrot_df)
mandelbrot_df
```
```{r}
mandelbrot_df$Size_F = as.factor(mandelbrot_df$Size)
mandelbrot_df$Type_F = as.factor(mandelbrot_df$Type)
```


```{r}
ggplot() +               
  geom_line(data = mandelbrot_df, aes(x=Size, y=Time, color = Type))
```

We can see from this plot that the parallel performs way worse than in the other machines which is weird since this machine has more cores than the previous machines and we also made measurements at three different times to reduce impact of the time of experiment in which we could have the machine under heavy load. 

```{r}
reg_test_mandelbrot = lm(Time ~ 1, data = mandelbrot_df)
step(reg_test_mandelbrot, scope=Time~Size+Type_F, direction="forward")
```

```{r}
reg_mandelbrot = lm(Time~Size+Type_F, data=mandelbrot_df)
summary(reg_mandelbrot)
anova(reg_mandelbrot)
```

Here too the model took just the parallel type and size features as most important but the coefficients we have for the features are quite different so we can say that the machine in which we run the experiments impacts the results, so we will now merge the data from both machines to reduce the impact of the machine on the results

```{r}
merged_df = union(local_df, mandelbrot_df)
dplyr::glimpse(merged_df)
merged_df
```

```{r}
ggplot() +               
  geom_line(data = merged_df, aes(x=Size, y=Time, color = Type))
```

We can see that the parallel execution has a lot of variability due to the big difference between the two machines when it comes to the parallel execution of the algorithm.

```{r}
with(local_df, plot(Time~Size_F))
with(local_df, plot(Time~Type_F))
```

We can see here that we still have a lot of outliers.

```{r}
reg_test_merged = lm(Time ~ 1, data = merged_df)
step(reg_test_merged, scope=Time~Size+Type_F, direction="forward")
```

Here too the model takes into account the types and the size which is good and gives the highest coefficient to the parallel type which matches with the data in which the parallel algorithm has very different execution times compared to sequential and built-in types.

```{r}
reg_merged = lm(Time~Size+Type_F, data=merged_df)
summary(reg_merged)
anova(reg_merged)
```

The R squared is good but is less than the previous models' which is probably due to the merged data having higher variability so more unexplained parts by the model.

```{r}
pred = predict(reg_merged, data.frame(Size=300000, Type_F="Parallel"),interval="prediction")
pred
```
```{r}
merged_confidence = 0.1752284+0.0302271
merged_confidence
```
```{r}
confidence_interval
```

Although we used more data to fit this model we still got a higher confidence interval than the model fitted with the original dataset which could be explained by the high variability of the data we have now.


```{r}
par(mfcol=c(2,2))
plot(reg_merged)
plot(reg_merged, 4)
```

We see can from the cook's distance plot that there are a lot of outliers in the data which will result in a higher variance which makes it harder to fit a good model. We can also see from the residuals plot that the higher the execution time the higher the error which mostly happens when we have bigger sizes and this is were there are big differences between results depending on the type of the algorithm.

```{r}
size_pred <- seq(100,1000000,by=5000)
type_pred <- rep(c(1), length(size_pred))

par_time_pred <- reg_merged$coefficients[1]+reg_merged$coefficients[2]*size_pred+reg_merged$coefficients[3]
seq_time_pred <- reg_merged$coefficients[1]+reg_merged$coefficients[2]*size_pred+reg_merged$coefficients[4]
builtin_time_pred <- reg_merged$coefficients[1]+reg_merged$coefficients[2]*size_pred

par_fct_reg <- data.frame(size_pred=size_pred, time_pred=par_time_pred)
seq_fct_reg <- data.frame(size_pred=size_pred, type_pred=type_pred, time_pred=seq_time_pred)
builtin_fct_reg <- data.frame(size_pred=size_pred, type_pred=type_pred, time_pred=builtin_time_pred)

ggplot()+
geom_point(data=merged_df,aes(x=Size,y=Time, color = Type))+ geom_line(data=par_fct_reg,aes(x=size_pred,y=time_pred),col="green")+
geom_line(data=seq_fct_reg,aes(x=size_pred,y=time_pred),col="blue")+
geom_line(data=builtin_fct_reg,aes(x=size_pred,y=time_pred),col="red")+
stat_smooth(method="lm",se=FALSE)+
xlab("Size")+
ylab("Time")
```

From the above plot we can see that the model predicts higher execution times even for higher sizes for the parallel quick sort algorithm which contradicts the result of the original experiment in which the parallel had lower execution times for higher size values.

The model however does not really fit well the data for the parallel data points. We can improve this model by removing the outliers, however this is not the best solution as the experiment has a lot of flaws in the first place such as having a big gap between the sizes tested and the number of tests is still small, as well as the fact that the experiments order between the different algorithms is always the same. That's why we will design a new experiment.


## Using new designed experiment results 

Here I design a new experiment that addresses the flaws of the previous experiment. The improvements of this new experiment are:

- Random order of experiments between the different types
- Random order of array sizes in when testing
- Cover the whole space of values between 100 to 1000000
- Increase the number of tests done

To do that we will use an lhs design to generate different sizes between 100 and 1000000, since this design is best for covering a space as much as possible while not having a fixed spacing between the values, I created the script `scripts/gen_experiments.R` that generates these experiment and this the resulting dataset:


```{r}
experiment <- read.csv(file="data/experiments/qs_experiment.csv")
```

Check that we have same number of experiments for each type

```{r}
experiment$Type[experiment$Type == 1] <- "Parallel"
experiment$Type[experiment$Type == 0] <- "Sequential"
experiment$Type[experiment$Type == 2] <- "Built-in"
```


```{r}
experiment %>% group_by(Type) %>% count()
```

To see this randomization better we do this plot:

```{r}
experiment$order =  as.numeric(rownames(experiment))
```

```{r}
ggplot() +               
  geom_point(data = experiment, aes(x=Size, y=order, color = Type))
```

We can see that the Size space is better covered and randomized, and the order between the tests order is also well randomized.

I found that having 1000 for each type gives the best coverage of the size space, however when I run these experiments they take so long on my machine that's why I did this compromise of minimizing the number of experiments so that I can have the results within reasonable time.


## New experiment analysis

Let's analyse the new experiment results:

```{r}
experiment_results <- read.csv(file="data/debian_2023-02-03/measurements_16:10.csv")
experiment_results
```

```{r}
ggplot() +               
  geom_line(data = experiment_results, aes(x=Size, y=Time, color = Type))
```

This result is much better as we can see there's much less interpolation of the measurements and so more accurate evolutions of the measurements, as we can see there some outliers in which the time measurement is very high which could be due to a huge load on the machine at that time.

So to mitigate this, like we did in the previous experiment we will use measurements from different times:

```{r}
local_results <- list.files(path = "./data/debian_2023-02-03/", pattern = "*.csv", full.names = T) %>% map_df(~read_csv(.)) 
dplyr::glimpse(local_results)
local_results
```

```{r}
ggplot() +               
  geom_line(data = local_results, aes(x=Size, y=Time, color = Type))
```
We can see that we have less fluctuations than the previous graph, to improve this we should increase the time interval between generated measurements.

```{r}
local_results$Type_F = as.factor(local_results$Type)
with(local_results, plot(Time~Type_F))
```

We can see that we have big confidence intervals, and that's expected as the execution time for each algorithm changes with the size.


```{r}
mandelbrot_results <- list.files(path = "./data/im2ag-mandelbrot_2023-02-03/", pattern = "*.csv", full.names = T) %>% map_df(~read_csv(.)) 
dplyr::glimpse(mandelbrot_results)
mandelbrot_results
```

```{r}
ggplot() +               
  geom_line(data = mandelbrot_results, aes(x=Size, y=Time, color = Type))
```
```{r}
mandelbrot_results$Type_F = as.factor(mandelbrot_results$Type)
with(mandelbrot_results, plot(Time~Type_F))
```

Here again we have big execution times for the parallel algorithm in this machine, ideally we should check this in another machine since we have a big difference between the parallel execution time between the two machines, but for now I will continue the analysis we these results since for now I do not have acces to another machine.

```{r}
merged_results = union(local_results, mandelbrot_results)
dplyr::glimpse(merged_results)
merged_results
```

```{r}
ggplot() +               
  geom_line(data = merged_results, aes(x=Size, y=Time, color = Type))
```

As expected the big difference in parallel execution times between the machines results in having a lot of flectuations in the parallel algorithm graph, which will make it more difficult for the model to fit values for the parallel type.

Let's try to fit a model with this data:

```{r}
reg_test_results = lm(Time ~ 1, data = merged_results)
step(reg_test_results, scope=Time~Size+Type_F, direction="forward")
```

So the best model is the one that has both type and size in it, which is right.

```{r}
reg_results = lm(Time~Size+Type_F, data=merged_results)
summary(reg_results)
anova(reg_results)
```

The R squared is worse than the one from previous experiment, this could probably be due to having more data in this experiment. The residual standard error is still small which is good, and the F statistic is higher as now the model gives more importance to the sequential type feature as opposed to the previous model, however the intercept is not considered important and this could be due to the fact that the execution times of both the sequential and built-in are very similar so the model can explain the variability of the built-in type with the sequential type.

```{r}
pred = predict(reg_results, data.frame(Size=300000, Type_F="Parallel"),interval="prediction")
pred
```

```{r}
results_confidence = 0.3082128-0.07831349
results_confidence
```

```{r}
merged_confidence
```

Although we used more data to fit this model we still got similar confidence interval than the previous model which could be explained by a high variability of this data.


```{r}
par(mfcol=c(2,2))
plot(reg_results)
plot(reg_results, 4)
```

The Residuals vs fitted plots are much better than the ones in the previous model as we can see we the residuals do not have a particular structure and they are evenly distributed over the x-axis, the cook's distance plot is also better as most points are clustered in one region except some few outliers that are far. However the Normal Q-Q plot is worse as a lot of residuals are not normally distributed.


```{r}
size_pred <- seq(100,1000000,by=5000)
type_pred <- rep(c(1), length(size_pred))

par_time_pred <- reg_merged$coefficients[1]+reg_results$coefficients[2]*size_pred+reg_results$coefficients[3]
seq_time_pred <- reg_merged$coefficients[1]+reg_results$coefficients[2]*size_pred+reg_results$coefficients[4]
builtin_time_pred <- reg_merged$coefficients[1]+reg_results$coefficients[2]*size_pred

par_fct_reg <- data.frame(size_pred=size_pred, time_pred=par_time_pred)
seq_fct_reg <- data.frame(size_pred=size_pred, type_pred=type_pred, time_pred=seq_time_pred)
builtin_fct_reg <- data.frame(size_pred=size_pred, type_pred=type_pred, time_pred=builtin_time_pred)

ggplot()+
geom_point(data=merged_results,aes(x=Size,y=Time, color = Type))+ geom_line(data=par_fct_reg,aes(x=size_pred,y=time_pred),col="green")+
geom_line(data=seq_fct_reg,aes(x=size_pred,y=time_pred),col="blue")+
geom_line(data=builtin_fct_reg,aes(x=size_pred,y=time_pred),col="red")+
stat_smooth(method="lm",se=FALSE)+
xlab("Size")+
ylab("Time")
```

We can see that the model does fit well the sequential and built-in types, however for the parallel type the fit is bad which gives higher errors in the parallel type which explains why the residuals are not normally distributed since the residuals of parallel type will be so different from those of the other types so they are the outliers that causes the residuals to not be normally distributed.

To fix this we should run our experiment on some other machines and compare with the results of the machines we used here to see which one is the outlier to discard.



